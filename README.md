# SmartDQ

## Project Overview 

ReDaQ Assistenzsystem is a Flask-based web application integrated with Rasa Chatbot, designed to help small and medium-sized enterprises (KMU) evaluate data quality and recommend suitable Data Analytics and Predictive Analytics methods.

## Structure
```
redaq/
│── .gitignore          # Git ignore file
│── requirements.txt    # List of dependencies
│── config.py           # Flask configuration settings
│── run.py              # Main entry point for the Flask server
│── init_db.py          # DB initialization script
│── together_server.py  # Lightweight LLM interface server (to Together.ai)
│── babel.cfg           # i18n config
│── messages.pot        # Translation template
│── README.md           # Project documentation (this file)
│
│── instance/           # Local environment variables (e.g., database)
│── migrations/         # Database migration files
│── models/             # ML or DB models
│── tests/              # Unit tests
│── rasa/               # Rasa project (NLU, Core, actions)
│
│── app/                # Main Flask application
│   │── __init__.py     # Flask app initialization
│   │── models.py       # Database models
│   │── routes.py       # Main page routes
│   │── templates/      # HTML templates
│   │── static/         # CSS, JavaScript, and image files
│   │── translations/   # Language support files
│   │── data/           # Miscellaneous or uploaded data
│   │── data_quality/   # Data quality assessment module
```

## Explanation of Key Components

- **.gitignore** → A list of files we don't want to track in Git, such as passwords or temporary files.
- **requirements.txt** → Lists all the Python libraries required for this project. You can install them using `pip install -r requirements.txt`.
- **config.py** → Stores application-wide configuration settings like database connection.
- **run.py** → Main script that starts the Flask web server.
- **init_db.py** → Initializes the database schema.
- **together_server.py** → Hosts a local endpoint (`/chat`) that connects to Together.ai’s LLaMA model.
- **babel.cfg / messages.pot** → Used for i18n/language support.
- **instance/** → Stores local settings that should not be version-controlled.
- **migrations/** → Tracks changes to the database schema.
- **models/** → ML models or shared database schemas.
- **tests/** → Unit tests for app features.
- **rasa/** → Contains NLU training data, stories, rules, domain file, actions, and configs for the chatbot.
- **app/** → Contains all web frontend logic including routes, templates, static files, and data quality modules.
  - `translations/` → Stores localized message files.
  - `templates/` → Contains HTML templates like `chatbot.html`.
  - `static/` → CSS, JS, images for frontend.
  - `data_quality/` → Code related to data quality computation and logic.


## LLM Integration: Rasa Pro + Together.ai

This guide helps developers quickly understand and run the Rasa Pro-based chatbot used in the ReDaQ Assistenzsystem project. It is designed for easy onboarding and local development.

### 1. What is Together.ai (in this project)?

In this project, we use [Together.ai](https://together.ai) as a free and easy way to access open-source large language models (LLMs), specifically:

> **`meta-llama/Llama-Vision-Free` (LLaMA 3.2, 11B, Instruct)**

Instead of hosting a large model locally (which requires a GPU with >20GB VRAM), Together.ai lets us send a prompt over an API and receive a smart answer, similar to how GPT-based services work.

We chose this setup because:

- **LLaMA 3.2 11B Free**: One of the strongest free-access open-source LLMs available as of 2025
- **No GPU needed locally**: All inference runs on Together.ai's servers
- **Free Tier available**: Useful for academic/demo purposes without extra cost
- **Easy to integrate**: We wrapped the Together API inside a lightweight Flask server (`together_server.py`), so Rasa can interact with it via HTTP.

In this chatbot, all smart answers (e.g., explaining “What is data quality?”) are generated by this remote LLaMA model via Together.ai.

---

### 2. What is Rasa Pro?

[Rasa Pro](https://rasa.com/product/rasa-pro/) is the commercial edition of the Rasa open-source framework. It offers advanced features such as:

- **LLM integration support**
- **Built-in LLM policies**
- **Scalable architecture for enterprises**

In this project, we use Rasa Pro to enable integration with large language models (LLMs) like LLaMA 3.2 via Together.ai.

---

### 3. Project Structure Overview

```
rasa/
├── actions/              # Custom Python actions
├── data/
│   ├── nlu.yml           # User intents & examples
│   ├── rules.yml         # Rule-based dialogue patterns
│   ├── stories.yml       # Dialogue flow examples
├── domain.yml            # Defines intents, entities, slots, responses
├── config.yml            # Pipeline & policy configuration
├── models/               # Trained model files (excluded from git)
├── endpoints.yml         # Endpoint configuration (action server)
```

### 4. Install Rasa Pro

Rasa Pro requires a license key. You can install it using [uv](https://github.com/astral-sh/uv) or pip.

First, activate your virtual environment and set your license:

```bash
export RASA_PRO_LICENSE="your_license_token"
```

Then install Rasa Pro:

```bash
uv pip install rasa-pro
```

> For full installation instructions and requirements, see the official Rasa Pro guide: https://rasa.com/docs/pro/installation/python

### 5. Training the Chatbot

Train the Rasa model (must be done before using the chatbot):

```bash
cd rasa/
rasa train
```

This generates a `.tar.gz` file inside `rasa/models/`.

### 6. Running the Chatbot Locally (Socket.IO for Webchat)

#### Terminal 1 – Run the action server:
```bash
rasa run actions
```

#### Terminal 2 – Run the main Rasa server:
```bash
rasa run --enable-api --cors "*" --debug --connector socketio
```

This allows the Rasa chatbot to communicate with the web interface through the `rasa-webchat` component.


### 7. How Rasa Pro connects to Together.ai

To enable smart responses from an open-source LLaMA model, the Rasa Pro chatbot is connected to the Together.ai inference API via a custom `action_call_llama`.

#### Requirements:

- **Rasa Pro license key**  
  You need to export it as an environment variable:

  ```bash
  export RASA_PRO_LICENSE="your_license_token"
  ```

- **Together API key**  
  Used by the Flask-based `together_server.py` to access the LLaMA model:

  ```bash
  export TOGETHER_API_KEY="your_together_api_key"
  ```

#### Startup: 4-Terminal Workflow

Please run the following commands **in this exact order**, each in a separate terminal window:

##### 1. Terminal 1 – Start the Rasa **action server**

```bash
cd rasa
export RASA_PRO_LICENSE="your_license_token"
rasa run actions
```

##### 2. Terminal 2 – Start the Rasa **main server** (with socket support for webchat)

```bash
cd rasa
export RASA_PRO_LICENSE="your_license_token"
rasa run --enable-api --cors "*" --debug --connector socketio
```

##### 3. Terminal 3 – Start the Together **LLM Flask server**

```bash
export TOGETHER_API_KEY="your_together_api_key"
python together_server.py
```

Ensure port `5056` is not occupied. It provides the `/chat` endpoint for LLM queries.

##### 4. Terminal 4 – Start the **Flask web frontend**

```bash
python run.py
```

Then open `http://localhost:5000/chatbot` in your browser to interact with the chatbot.

#### Conversation Flow:

1. User message → `rasa-webchat`
2. Message hits Rasa (port `5005`)
3. Rasa triggers `action_call_llama`
4. That action sends a prompt to the Flask server (port `5056`)
5. Flask server sends it to Together.ai’s LLaMA endpoint
6. LLM reply returns → Rasa → Frontend

---

### Notes

- `rasa/models/` and `.rasa/` are excluded from version control via `.gitignore`
- Use `rasa data validate` to check training files for errors
- To share trained models, upload the `.tar.gz` file via tubCloud or other cloud service


## TO DO

### Add LLaMA 3.2 fallback for undefined Rasa NLU intents

Enable Rasa to call LLaMA 3.2 when user input does not match any defined NLU intent.

### Evaluate Rasa + RAGFlow vs. Rasa + LLaMA 3.2

Compare two chatbot backends to decide which better fits our assistant system:

1. Rasa + RAGFlow (document-based Q&A with structured retrieval)
2. Rasa + LLaMA 3.2 (LLM-powered dialogue generation)

Decide based on: setup complexity, maintenance effort, and use case coverage

### Evaluate if Botpress can replace the current chatbot setup

Assess whether Botpress is a viable alternative to the current Rasa-based chatbot architecture.

1. Can Botpress handle structured, guided conversations (e.g., for maturity assessment)?
2. Can it connect to external services like RAGFlow, Neo4j, or custom Python APIs?
3. Does it support LLM integration (e.g., with LLaMA 3.2 or OpenAI)?
4. How flexible is Botpress in terms of multilingual support and UI embedding?
5. What are the data privacy implications (self-hosting, GDPR compliance)?
6. Is Botpress easier to maintain or scale than Rasa?

